{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_input.shape:  (64, 168, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "input_x = tf.placeholder(tf.float32, [64,192, 3+1], name = 'input_x')\n",
    "cnn_input = input_x[:, :168, 0] #shape = [batch_size, encode_length], pure data\n",
    "cnn_input = tf.expand_dims(cnn_input, axis=2)\n",
    "cnn_input = tf.expand_dims(cnn_input, axis=3) #[batch_size, encode_length, width=1, input_channel=1]\n",
    "print (\"cnn_input.shape: \", cnn_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 168, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Conv ###\n",
    "\n",
    "filter_size_list = [3, 3, 3, 3, 3] # 看前后\n",
    "output_channel_list = [80,60,40,40,40]\n",
    "input_channel_list = [1,80,60,40,40]\n",
    "cnn_output_list = []\n",
    "cnn_input_list = []\n",
    "cnn_input_list.append(cnn_input)\n",
    "print((cnn_input_list[0].shape))\n",
    "batch_size = 64\n",
    "dialations_size = [1, 3, 6, 12, 24]\n",
    "padding_size = [2, 6, 12, 24, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pad.shape:  (64, 2, 1, 1)\n",
      "intput.shape:  (64, 168, 1, 1)\n",
      "batch_input_conv_pad.shape:  (64, 170, 1, 1)\n",
      "h.shape:  (64, 168, 80)\n",
      "\n",
      "pad.shape:  (64, 6, 1, 80)\n",
      "intput.shape:  (64, 168, 1, 80)\n",
      "batch_input_conv_pad.shape:  (64, 174, 1, 80)\n",
      "h.shape:  (64, 168, 60)\n",
      "\n",
      "pad.shape:  (64, 12, 1, 60)\n",
      "intput.shape:  (64, 168, 1, 60)\n",
      "batch_input_conv_pad.shape:  (64, 180, 1, 60)\n",
      "h.shape:  (64, 168, 40)\n",
      "\n",
      "pad.shape:  (64, 24, 1, 40)\n",
      "intput.shape:  (64, 168, 1, 40)\n",
      "batch_input_conv_pad.shape:  (64, 192, 1, 40)\n",
      "h.shape:  (64, 168, 40)\n",
      "\n",
      "pad.shape:  (64, 48, 1, 40)\n",
      "intput.shape:  (64, 168, 1, 40)\n",
      "batch_input_conv_pad.shape:  (64, 216, 1, 40)\n",
      "h.shape:  (64, 168, 40)\n",
      "===============\n",
      "cnn_output.shape:  (64, 168, 40)\n"
     ]
    }
   ],
   "source": [
    "for i, filter_size in enumerate(filter_size_list):\n",
    "    with tf.variable_scope(\"CNN-\"+str(filter_size)):\n",
    "        print()\n",
    "        num_padding = padding_size[i]\n",
    "        pad = tf.zeros([batch_size, num_padding, 1, input_channel_list[i]], tf.float32)\n",
    "        print(\"pad.shape: \",pad.shape)\n",
    "        print(\"intput.shape: \",cnn_input_list[i].shape)\n",
    "        batch_input_conv_pad = tf.concat([pad, cnn_input_list[i]], 1) #[batch_size, encode_length+filter_size-1, width=1, input_channel=1]\n",
    "        print (\"batch_input_conv_pad.shape: \", batch_input_conv_pad.shape)\n",
    "        #print (\"cnn_input.shape: \", cnn_input.shape)\n",
    "        filter_shape = [filter_size, 1, input_channel_list[i], output_channel_list[i]]\n",
    "        # parameter for conv\n",
    "        W = tf.Variable(\n",
    "            tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "        b = tf.Variable(\n",
    "            tf.constant(0.1, shape = [output_channel_list[i]]), name='b')\n",
    "        conv = tf.nn.conv2d(\n",
    "            batch_input_conv_pad,  ### shape = [batch_size, encode_length]\n",
    "            W,\n",
    "            strides=[1,1,1,1],\n",
    "            padding = 'VALID', #不进行padding，多余部分丢弃\n",
    "            name='conv',\n",
    "            dilations = [1,dialations_size[i],1,1]\n",
    "            #dilations = [1,1,1,1]\n",
    "            ) #conv.shape: [batch_size, sequence_length(因为stride是1),1, num_filters(也就是feature maps的个数)]\n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv,b), name = 'relu') # [batch_size, encode_length, width=1, ouput_channel = 40]\n",
    "        cnn_input_list.append(h)\n",
    "        h = tf.squeeze(h, [2]) # [batch_size, encode_length, ouput_channel = 40]\n",
    "        cnn_output_list.append(h)\n",
    "        print (\"h.shape: \", h.shape)\n",
    "#cnn_output = tf.concat(cnn_output_list, axis=2) #[batch_size, encode_length, num_filter*4=40]\n",
    "cnn_output = cnn_output_list[-1]\n",
    "print (\"===============\\ncnn_output.shape: \", cnn_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
